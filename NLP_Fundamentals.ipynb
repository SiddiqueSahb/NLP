{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ad7996-2eeb-429a-b340-6e1d0c75d6d4",
   "metadata": {},
   "source": [
    "# NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d852a60-0d55-4bbe-98d1-b77cf95e6afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7822e81c-124f-4697-92a1-b73d55487a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Example corpus\n",
    "corpus = \"Global temperatures reached a new high this year. Scientists are concerned. Governments are urged to act.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0a7f1-b1e0-4d0f-b33a-dea50505a6b7",
   "metadata": {},
   "source": [
    "## Sentence Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf17e16f-15e0-4615-8529-2d2be4b3d405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Global temperatures reached a new high this year.', 'Scientists are concerned.', 'Governments are urged to act.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Tokenizing sentences\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "# Step 5: Print the tokenized sentences\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f600a2a-ab86-4599-a516-78bb746b1373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global temperatures reached a new high this year.\n",
      "Scientists are concerned.\n",
      "Governments are urged to act.\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f57318-3d56-4212-bc7b-2a823694f566",
   "metadata": {},
   "source": [
    "## Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b80152e-ead1-4c9f-8fa0-97614ab24e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd2641b0-3b00-43e8-beb3-92324c58b841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Global',\n",
       " 'temperatures',\n",
       " 'reached',\n",
       " 'a',\n",
       " 'new',\n",
       " 'high',\n",
       " 'this',\n",
       " 'year',\n",
       " '.',\n",
       " 'Scientists',\n",
       " 'are',\n",
       " 'concerned',\n",
       " '.',\n",
       " 'Governments',\n",
       " 'are',\n",
       " 'urged',\n",
       " 'to',\n",
       " 'act',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d2b1b6-22eb-48e5-856e-5f004091d2d7",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b258d282-783a-4047-b988-03d28069383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e590f11a-7519-4d26-9183-d423203b511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4d8f7d0-a664-40da-b602-3c787a10a5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999a1bb-03d2-49a5-be3f-262f717d0fe2",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8aaaf9-59f3-4d99-a699-1b73721fb99c",
   "metadata": {},
   "source": [
    "## Text Preprocessing - Stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff681971-2b44-4a27-8656-065f78396734",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech Of DR APJ Abdul Kalam\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b8322a5-38e2-4610-82e3-ecf443a767de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7e69b8f-8508-48fc-9684-48f36f4a2a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0190aa8-52aa-438c-8f3b-e6ebe6de0176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5109e81d-08c4-4c0b-be7a-de3eb5c07337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1eeaab88-4b7a-4c61-b4b5-9858f162c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d4bd2b3-c7b2-4151-aa0e-945e51b9f87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure required resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b289c470-5488-441b-964e-171d3a800a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    #word tokenize\n",
    "    words = nltk.word_tokenize(sentences[i]) \n",
    "    #for each word , if word is not a stem word then find its stem word\n",
    "    words = [stemmer.stem(word) for word in words if word.lower() not in set(stopwords.words('english'))]    \n",
    "    sentences[i ]= ' '.join(words)# converting all the list of words into sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa4ce1c9-bc03-4bbd-8196-43bad8809a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india .',\n",
       " '3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'conquer anyon .',\n",
       " 'grab land , cultur , histori tri enforc way life .',\n",
       " '?',\n",
       " 'respect freedom others.that first vision freedom .',\n",
       " 'believ india got first vision 1857 , start war independ .',\n",
       " 'freedom must protect nurtur build .',\n",
       " 'free , one respect us .',\n",
       " 'second vision india ’ develop .',\n",
       " 'fifti year develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top 5 nation world term gdp .',\n",
       " '10 percent growth rate area .',\n",
       " 'poverti level fall .',\n",
       " 'achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " '’ incorrect ?',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'believ unless india stand world , one respect us .',\n",
       " 'strength respect strength .',\n",
       " 'must strong militari power also econom power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'lucki work three close consid great opportun life .',\n",
       " 'see four mileston career']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407d895-3253-40e7-9681-9e8b56657871",
   "metadata": {},
   "source": [
    "## Using Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7cde5b4-beb2-4796-bb03-d2d2b3c94d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c511f3dc-8947-4048-92a4-278686de60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "for i in range(len(sentences)):\n",
    "    #sentences[i]=sentences[i].lower()\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word.lower()) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bbc5b03-1981-4a74-9032-e6dff19de6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year history , people world come invaded u , captured land , conquered mind .',\n",
       " 'from alexander onwards , greek , turk , mogul , portuguese , british , french , dutch , came looted u , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquered anyone .',\n",
       " 'we grabbed land , culture , history tried enforce way life .',\n",
       " 'why ?',\n",
       " 'because respect freedom others.that first vision freedom .',\n",
       " 'i believe india got first vision 1857 , started war independence .',\n",
       " 'it freedom must protect nurture build .',\n",
       " 'if free , one respect u .',\n",
       " 'my second vision india ’ development .',\n",
       " 'for fifty year developing nation .',\n",
       " 'it time see developed nation .',\n",
       " 'we among top 5 nation world term gdp .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverty level falling .',\n",
       " 'our achievement globally recognised today .',\n",
       " 'yet lack self-confidence see developed nation , self-reliant self-assured .',\n",
       " 'isn ’ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'because i believe unless india stand world , one respect u .',\n",
       " 'only strength respect strength .',\n",
       " 'we must strong military power also economic power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortune worked three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeeded dr. brahm prakash , father nuclear material .',\n",
       " 'i lucky worked three closely consider great opportunity life .',\n",
       " 'i see four milestone career']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74ada50c-4b36-472c-9e08-88d0f67cd8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc03cdc6-dcad-4f60-ab18-8afc113411a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eat\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->history\n",
      "finally---->finally\n",
      "finalized---->finalize\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b16c6-2239-4bfd-8542-5fa7d7c9a3a0",
   "metadata": {},
   "source": [
    "## BAG OF WORDS WITH LEMMETIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e4f99-d746-45f8-bab5-291b9364337d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1364a2fd-65b2-4b05-a779-ff8ad9da5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9954ac7e-3625-40ce-82e3-44bf0997a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d04cd30-e566-4ff0-b131-f6207138d9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561f045-1450-4152-9640-d5a0acfcbbc9",
   "metadata": {},
   "source": [
    "## TextPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71e5d75d-ab64-4566-b373-7986c73bedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"The cats are playing in the garden.\",\n",
    "    \"A dog was barking loudly in the yard.\",\n",
    "    \"Children love playing with their pets in the park.\"\n",
    "]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def textPreprocessing(text):\n",
    "    #tokenize the words\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    #lemmatize and remove stop words\n",
    "    lemmatizeWords = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(lemmatizeWords)\n",
    "\n",
    "preProcessedCorpus = [textPreprocessing(sent) for sent in corpus]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d903b9-7ba3-41d5-92c3-f8663297efa5",
   "metadata": {},
   "source": [
    "## Creating Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14eb0f3d-0116-4bdb-a026-8939866226df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(preProcessedCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1e92b-b230-4100-9637-e143d6f3b99e",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5b95528-459a-4d02-a7b1-891967d97b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Vocabulary):\n",
      "['barking' 'cat' 'child' 'dog' 'garden' 'loudly' 'love' 'park' 'pet'\n",
      " 'playing' 'yard']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display results\n",
    "print(\"Feature Names (Vocabulary):\")\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "962eb2fb-12be-4150-a1e6-4a95e753a9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words Matrix:\n",
      "[[0 1 0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 1 0 1 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nBag of Words Matrix:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7658f164-55ba-4705-a78d-4cced12e16a7",
   "metadata": {},
   "source": [
    "## N GRAM AND TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e40442bf-c16d-49a7-8cf1-c99c07be918d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Vocabulary) with N-grams:\n",
      "['barking' 'barking loudly' 'cat' 'cat playing' 'child' 'child love' 'dog'\n",
      " 'dog barking' 'garden' 'loudly' 'loudly yard' 'love' 'love playing'\n",
      " 'park' 'pet' 'pet park' 'playing' 'playing garden' 'playing pet' 'yard']\n",
      "\n",
      "Bag of Words Matrix with N-grams:\n",
      "[[0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0]\n",
      " [1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0]]\n",
      "\n",
      "Feature Names (Vocabulary) with TF-IDF:\n",
      "['barking' 'barking loudly' 'cat' 'cat playing' 'child' 'child love' 'dog'\n",
      " 'dog barking' 'garden' 'loudly' 'loudly yard' 'love' 'love playing'\n",
      " 'park' 'pet' 'pet park' 'playing' 'playing garden' 'playing pet' 'yard']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.         0.         0.46735098 0.46735098 0.         0.\n",
      "  0.         0.         0.46735098 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.35543247 0.46735098\n",
      "  0.         0.        ]\n",
      " [0.37796447 0.37796447 0.         0.         0.         0.\n",
      "  0.37796447 0.37796447 0.         0.37796447 0.37796447 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.37796447]\n",
      " [0.         0.         0.         0.         0.34142622 0.34142622\n",
      "  0.         0.         0.         0.         0.         0.34142622\n",
      "  0.34142622 0.34142622 0.34142622 0.34142622 0.25966344 0.\n",
      "  0.34142622 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Bag of Words with N-grams\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2))  # Unigrams and Bigrams\n",
    "X_ngram = vectorizer_ngram.fit_transform(preProcessedCorpus)\n",
    "\n",
    "# 2. TF-IDF Representation\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Unigrams and Bigrams\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(preProcessedCorpus)\n",
    "\n",
    "# Display Results\n",
    "print(\"Feature Names (Vocabulary) with N-grams:\")\n",
    "print(vectorizer_ngram.get_feature_names_out())\n",
    "\n",
    "print(\"\\nBag of Words Matrix with N-grams:\")\n",
    "print(X_ngram.toarray())\n",
    "\n",
    "print(\"\\nFeature Names (Vocabulary) with TF-IDF:\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf1f841-aa7e-46f2-9b7e-ebc80fed2db1",
   "metadata": {},
   "source": [
    "## POS and Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c86ca215-357d-4a20-a535-5cff0d77fac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (8.3.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
      "Requirement already satisfied: blis<1.2.0,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.1.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1e1f1e43-1eac-4044-a739-fe15e71c4a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.5.3)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic) (0.6.0)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "   ---------------------------------------- 0.0/431.8 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 92.2/431.8 kB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 317.4/431.8 kB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 431.8/431.8 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB 5.2 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.8/2.0 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.0 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.5/2.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.7/2.0 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.0 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, pydantic-core, pydantic\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.6\n",
      "    Uninstalling pydantic_core-2.14.6:\n",
      "      Successfully uninstalled pydantic_core-2.14.6\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.5.3\n",
      "    Uninstalling pydantic-2.5.3:\n",
      "      Successfully uninstalled pydantic-2.5.3\n",
      "Successfully installed pydantic-2.10.4 pydantic-core-2.27.2 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "41b2fd61-004f-4bbd-b77b-7682ccfe2a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b2018123-747c-4ea0-afc0-30262178de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb6da5f6-f06f-40d6-b6fa-d8e2bfd32412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags:\n",
      "Apple: PROPN (NNP)\n",
      "Inc.: PROPN (NNP)\n",
      "was: AUX (VBD)\n",
      "founded: VERB (VBN)\n",
      "by: ADP (IN)\n",
      "Steve: PROPN (NNP)\n",
      "Jobs: PROPN (NNP)\n",
      "in: ADP (IN)\n",
      "Cupertino: PROPN (NNP)\n",
      ",: PUNCT (,)\n",
      "California: PROPN (NNP)\n",
      ".: PUNCT (.)\n",
      "\n",
      "Named Entities:\n",
      "Apple Inc.: ORG (Companies, agencies, institutions, etc.)\n",
      "Steve Jobs: PERSON (People, including fictional)\n",
      "Cupertino: GPE (Countries, cities, states)\n",
      "California: GPE (Countries, cities, states)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# POS Tagging\n",
    "print(\"POS Tags:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.pos_} ({token.tag_})\")\n",
    "\n",
    "# Named Entity Recognition\n",
    "print(\"\\nNamed Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14e55e-3090-4d0c-a6b6-3ffa17bd1db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
